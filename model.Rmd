---
title: "Southend planning applications - A study"
author: "Alex Dolphin"
date: 13/10/2019
output:
  html_document:
    df_print: paged
    code_folding: hide
    toc: true
    toc_float: true
---

```{r libraries, echo=TRUE, message=FALSE}
library(caret)
library(data.table)
library(ggplot2)
library(gender)
library(magrittr)
library(Matrix)
library(ModelMetrics)
library(neuralnet)
library(tm)
library(xgboost)
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='figures/', echo=TRUE, warning=FALSE, message=FALSE)
```

# Introduction

In order to commence some sort of architectural project within Southend borough, an approved planning application is required. Examples of projects can be seen on the [Southend Borough Council Planning Application Website](https://www.southend.gov.uk/info/200158/common_projects).

This study aims to look at previously decided applications and their respective details, and see if we can learn what is required for a successful application. In fact we will look at it from a reverse perspective, i.e. what features of an application may lead to it being rejected.

Through this study we hope to:

- Learn which features of an application are relevant for rejection
- Build a model that can predict whether or not an application will be rejected

# The data

The data was sourced from the [planning applications search portal](https://publicaccess.southend.gov.uk/online-applications/search.do?action=monthlyList) of Southend Borough Council website. [A Python library](https://github.com/thecasper2/southend-planning-scraper) was constructed in order to scrape applications that had been decided between September 2018 and October 2019. The data contains, among other information:

- The decision of the application
- Details about the nature of the application
- A text description of the application

```{r read_data, echo=TRUE}
data_read <- data.table()
for(filename in list.files(path="data", pattern="details")){
    temp_file <- read.csv(paste0("data/", filename)) %>% as.data.table()
    data_read <- rbind(data_read, temp_file)
}
data_read[, X:=NULL]
data_read <- data_read[Decision != "Withdrawn"]
```

## Building features

In order to learn what is important for the rejection of an application, we need to build features. This means constructing meaningful data from the raw data we currently have. This should become clearer in the subsections

```{r transfer_df, echo=TRUE}
# Transfer to new variable
data <- data_read
```

### Defining the target

Firstly we need to determine what our target is, i.e. what we are concretely trying to predict. This may seem trivial, as we have already stated that we are interested in predicting acceptance or rejection. Practically the decision actually takes a number of forms, e.g. "Refuse Non Material Amendment", or "Grant consent to carry out work to trees". 

```{r target, echo=TRUE}
# Label
success <- paste(c("Allow", "Non Conditional", "Grant", "No Objection"), collapse="|")
failure <- paste(c("Refusal", "Refuse"), collapse="|")
data[grepl(success, Decision), label:= 0]
data[grepl(failure, Decision), label:= 1]
data <- data[label %in% c(0, 1)]
```

To make it simple we will take the decision text and assign the following labels:

- 0 if the label contains `r print(success)`
- 1 if the label contains `r print(failure)`

Where 0 denotes a successful application, and 1 denotes a rejection. This may seem counter intuative, but we see in the next section why this is important.

#### Target skew

One challenge that occurs when building a prediction is target skew, i.e. our target isn't evenly distributed across applications. Here we see in our data that most applications have target 0, i.e. most applications are successful:

```{r label_skew, echo=TRUE}
ggplot(data[, c("label")], aes(x=as.factor(label))) +
    geom_histogram(stat="count") +
    theme_bw() +
    labs(x="Label", y="Count", title="Distribution of target labels", subtitle="Check for skewness")
```

This is important because it makes it simpler for a machine-learnt algorithm to do well. If it simply predicts that every application is approved then it will have a fairly good success rate. However this doesn't bring us much information or practical use.

If we define the target to be the sparse value (i.e. the less-frequent outcome, a rejection) we can use an evaluation metric called "F1 score" to punish an algorithm for predicting all one value. This metric is explained in more detail in a later section

### Case officer sex

This feature will take the case officer name and determine if they are male or female (with some probability).

```{r case_officer_sex, echo=TRUE}
# Case officer sex
case_officers <- data.table(Case.Officer=levels(data$Case.Officer))
case_officers[, name := gsub(" .*$","",as.character(Case.Officer))]
case_officer_sexes <- as.data.table(gender(unique(case_officers$name)))
case_officers <- case_officers[case_officer_sexes, on="name"]
case_officers[, Case.Officer.Male := proportion_male]
case_officers <- case_officers[, c("Case.Officer", "Case.Officer.Male")]
case_officers[, Case.Officer := as.factor(Case.Officer)]
data <- data[case_officers, on="Case.Officer"]
```

### Applicant sex

This feature will take the applicant's title and determine if they are male or female. If there are a couple (e.g. Mr and Mrs) we will label it a couple. If it doesn't match any of these conditions, we label is as "other".

```{r applicant_sex, echo=TRUE}
# Applicant sex
male <- paste(c("Mr "), collapse="|")
female <- paste(c("Mrs ", "Miss", "Ms "), collapse="|")
couple <- paste(c("Mr & Mrs", "Mr And Mrs"), collapse="|")
data[grepl(male, Applicant.Name), Applicant.Sex := "male"]
data[grepl(female, Applicant.Name), Applicant.Sex := "female"]
data[grepl(couple, Applicant.Name), Applicant.Sex := "couple"]
data[is.na(Applicant.Sex), Applicant.Sex := "other"]
```

### Using an agent

If the field "Agent.Name" is populated, then the applicant is using an agent for the application. In this case we set the feature to 1, otherwise we set it to 0.

```{r using_agent, echo=TRUE}
# Using agent
data[, Using.Agent := ifelse(Agent.Name == "", 0, 1)]
```

### Application description

The application description may be full of a lot of vital information. To be able to work with it cleanly we first need to remove punctuation, and "stop words". For example the word "the" doesn't bring us much value.

```{r clean_description, echo=TRUE}
# Strip description punctuation and stopwords
data[, description := gsub('[[:punct:] ]+',' ', description)]
data[, description := removeWords(tolower(description), stopwords("en"))]
```

#### Word importance

We turn our attention to the importance of words in the application's approval or rejection status. To do this we firstly note every instance of every word and count how many times it appears in a rejection, and how many times in an acceptance. If it appears multiple times in a single application, we only count it once.

```{r average_rejection}
# Determine average rejection rate
total_rejection <- nrow(data[label==1])
total_acceptance <- nrow(data[label==0])
```

What we know currently is that approximately `r print(round(total_rejection/total_rejection+total_acceptance))`% of applications are rejected. Given each word's prevalance in accepted and rejected applications, we can see if a word contributes more or less to a rejection. For this we construct the "rejection index" metric, where a value greater than 0 means a word is more likely to lead to a rejection, and less than 0 for less likely.

Given that this follows a binomial pattern (successes and failures), we can also perform a fisher's exact test on each word to see the significance (p value) in the difference between the word and the average rejection rate. We can then filter to words that contribute a significant difference. Note that a low p-value means high significance (see the [Wikipedia page for Fisher's exact test](https://en.wikipedia.org/wiki/Fisher%27s_exact_test) for more information).

```{r description_words, echo=TRUE}

# Make function to return word frequency data frame
make_Tdm <- function(string_vector){
    myTdm <- as.matrix(TermDocumentMatrix(Corpus(VectorSource(string_vector))))
    df <- data.frame(
        ST = rownames(myTdm), 
        Freq = rowSums(myTdm), 
        row.names = NULL
    )
    return(df[order(-df$Freq),] %>% as.data.table())
}

# Create reject and approve word frequencies
reject_word_frequency <- make_Tdm(data[label == 1]$description)
colnames(reject_word_frequency) <- c("ST", "reject_freq")
approve_word_frequency <- make_Tdm(data[label == 0]$description)
colnames(approve_word_frequency) <- c("ST", "accept_freq")
#word_frequency <- total_word_frequency[reject_word_frequency, on="ST"][approve_word_frequency, on="ST"]
word_frequency <- merge(approve_word_frequency, reject_word_frequency, on="ST", all = TRUE)
word_frequency[is.na(accept_freq), accept_freq := 0]
word_frequency[is.na(reject_freq), reject_freq := 0]
word_frequency[, total_freq := accept_freq + reject_freq]

# Get overll rejection ratio
word_frequency[, total_rejection := total_rejection]
word_frequency[, total_acceptance := total_acceptance]

# Compare to average rejection rate
word_frequency[, average_rejection_ratio := total_rejection/(total_rejection + total_acceptance)]
word_frequency[, reject_ratio := reject_freq/(reject_freq + accept_freq)]
word_frequency[, rejection_index := (reject_ratio/average_rejection_ratio)-1]

get_fisher_p_val <- function(a, b, c, d){
    return(fisher.test(rbind(c(a, b), c(c, d)), alternative = "two.sided", workspace = 200000)$p.value)
}

# Get significance of difference
word_frequency[,
    p_value := get_fisher_p_val(reject_freq, accept_freq, total_rejection, total_acceptance),
    by = seq_len(nrow(word_frequency))
]

# Plot rejection indicies
plot_word_frequency <- word_frequency[p_value < 0.0005 & total_freq > 100][order(-rejection_index)][,ST:=factor(ST, levels=ST)]
word_freq_plot <- ggplot(plot_word_frequency, aes(x=ST, y=rejection_index, alpha=-p_value, fill=total_freq)) +
    geom_bar(stat="identity") +
    coord_flip() +
    theme_bw() +
    scale_fill_gradient(low="blue", high="red")
word_freq_plot
```

#### Building the feature

Using our most relevant important words, we can then build features from each one. For each word we can construct a single feature called "Is.That.Word" (where That.Word is replaced by the word), and set it to 1 if the description contains it, and 0 otherwise.

```{r features, echo=TRUE}
# Description features
#word_features <- c("erect", "storey", "extension", "install", "alter", "roof", "parking")
word_features <- c("tree", "wall", "eaves", "vehicular", "cycle", "dwelling", "parking", "flats")

# Build 1 feature for each word
for(word in word_features){
    word_col <- paste0("Is.", word)
    # Ensure we look for complete words (not inside of another word)
    word <- paste0("\\<", word, "\\>")
    data[, (word_col) := ifelse(grepl(word, tolower(description)), 1, 0)]
}
```

# Preparing data for the model

In order to train a model we need to preprocess our data. The main step for this is treating "categorical" data, i.e. data that isn't numerical, but instead takes on particular values. One example is "Application.Type". The application can be a number of "types" in written form, which a model will not be able to learn from.

To deal with these we use a method called "one-hot encoding", where each type is a made into a new feature. That feature is then either said to be 0 or 1 for one application depending on if the application is that type. More information about one-hot encoding can be found [here](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f).

```{r test_train, echo=TRUE}
# Set seed
set.seed(123)

# Reorder
reordered_data <- data[order(rnorm(nrow(data)))]

# Determine feature data types
all_cols <- colnames(data)
binary_cols <- all_cols[grepl("Is.", all_cols)]

numerical_cols <- c(
    "Case.Officer.Male",
    "Using.Agent",
    "label",
    binary_cols
)
categorical_cols <- c(
    "Application.Type",
    "Case.Officer",
    "Ward",
    "Applicant.Sex"
)
reordered_data <- reordered_data[!is.na(Ward)]

# Build feature dataset, starting with numerical data
numerical_features <- reordered_data[, numerical_cols, with=FALSE]

# One-hot encode categorical variables
categorical_features <- data.table()
for(column in categorical_cols){
    one_hot <- reordered_data[[column]] %>% as.factor() %>% class2ind()
    categorical_features <- cbind(categorical_features, one_hot)
}

# And combine
data_features <- cbind(categorical_features, numerical_features)

# Remove any remaining NA
data_features <- na.omit(data_features)

# Clean up column names
colnames(data_features) <- make.names(colnames(data_features), unique=TRUE)

# Set label as factor
data_features[, label := as.factor(label)]
```

After that we need to split the data into training and test data. This means we can train a model on training data, and evaluate how well that model performs on new test data that it hasn't seen before. This helps us prevent the model from fitting the training data really well, but not generalising to new data very well.

```{r test_train_split}
# Split into train and test
random_split <- function(data, split){
    n <- nrow(data)
    chosen_indices <- head(seq(1, n)[order(rnorm(n))], split*n)
    return(list(
        data[chosen_indices],
        data[-chosen_indices]
    ))
}
split = 0.9
split_data <- random_split(data_features, split)
trainset <- split_data[[1]]
testset <- split_data[[2]]
```

We split the data into `r round(split*100)`% training data, and `r round((1-split)*100)`% test data.

## Defining success

In order to determine if a model is predicting well we need an evaluation metric. As mentioned above we wish to use F1 score. Briefly, this metric evaluates two things:

- Precision: are the predicted rejections actual rejections?
- Recall: how many actual rejections did we predict?

This is useful because it forces our model to not only reduce the false positives (i.e. predicting rejection when actually it was accepted), but also attempt to find the rejections (rather than prudently predict all accepted).

The F1 score ranges from:

- 0: No correct predictions
- 1: All predictions correct, and all rejections found

More information about F1 score can be found [here](https://en.wikipedia.org/wiki/F1_score).

```{r evaluation_function, echo=TRUE}
# Make a function to evaluate the neural net against some actual labels
renum <- function(vector){
    return(as.numeric(as.character(vector)))
}

evaluate_nn <- function(nn, data){
    predict <- compute(nn, data)
    predict_labels <- ifelse(predict$net.result[, 1] > predict$net.result[, 2], 0, 1)
    return(f1Score(renum(data$label), predict_labels))
}
```

In order to determine if our F1 score is "good", we can compare it to a baseline prediction method. We look at the F1 scores of two simple methods:

- Predicting at random (i.e. equal and random chance of approval and rejection)
- Predicting all rejected

For obvious reasons we would like to do better than either of these methods. Note that predicting all accepted would give an F1 score of 0.

```{r baseline_scores, echo=TRUE}
set.seed(123)
f1_random <- f1Score(renum(testset$label), round(runif(nrow(testset))))
f1_all_rejected <- f1Score(renum(testset$label), rep(1, nrow(testset)))
```

The F1 scores for the baseline methods are:

- Predicting at random: `r round(f1_random, 3)`
- Predicting all rejected: `r round(f1_all_rejected, 3)`

# Training a model
## Learning curves

We firstly want to do a sanity check, we wish to check that as we increase the size of our dataset, our F1 score on out-of-sample data (data we didn't use to train the model) improves.

```{r increasing_dataset, eval=FALSE, echo=TRUE}
set.seed(34)
score_by_length <- data.frame()
for(i in seq(100,2408,50)){
    f1_train <- c()
    f1_cv <- c()
    for(k in 1:10){
        split_cv <- 0.9
        trainset_cv <- random_split(trainset, split_cv)
        trainset_cv_subsample <- head(trainset_cv[[1]][order(rnorm(nrow(trainset_cv[[1]])))], i)
        nn_length <- neuralnet(
            label ~ .,
            data=trainset_cv_subsample,
            hidden=c(1,3),
            linear.output=FALSE,
            threshold=0.5,
            stepmax = 500000,
            #learningrate.limit = c(0.1, 1),
            rep=5
        )
        f1_train <- c(f1_train, evaluate_nn(nn_length, trainset_cv[[1]]))
        f1_cv <- c(f1_cv, evaluate_nn(nn_length, trainset_cv[[2]]))
        cat("\r", k, " fold")
    }
    f1_train <- mean(f1_train)
    f1_cv <- mean(f1_cv)
    score_by_length <- rbind(
        score_by_length, data.frame(length=i, train=f1_train, test=f1_cv)
    )
    cat(i, " rows")
}
plot_score <- melt(score_by_length, id="length")
ggplot(plot_score, aes(x=length, y=value, col=variable)) +
    geom_line() +
    theme_bw() +
    labs(x="Dataset size (rows)", y="Average neural network F1 score", col="Score type")
```

From the plot we see that increasing the datasize generally improves the models performance on both training and test data, which is reassuring. After a certain number of rows of data the score appears to approach a maximum theoretical value. Naturally the model is limited to the information that our features provide.

## Neural network

An artificial neural network (ANN) is a machine learning algorithm that, very loosely speaking, is based upon the the method of learning seen in the brains of animals. The mathematics behind an ANN are not particularly complicated, and practically it is not complex to train one. We will try to do this with our data.

More information on ANNs can be found [here](https://en.wikipedia.org/wiki/Artificial_neural_network).

### Training

To train an neural network we need to specify a number of hyperparameters, such as how quickly should the algorithm learn (too fast and we may not converge, too slow and we might not optimise), how many hidden layers and nodes we require (please see the link above for more information), and more.

To determine these parameters we can do a grid search, training a new algorithm each time. This means we select a range of options and train a new algorithm for each one. Each time we put aside a little of the training data, and then use it to evaluate the model afterwards. Once we determine the best parameters, we train the full model.

```{r train_neuralnet, echo=TRUE, message=FALSE}
set.seed(65)
# Set up some cross-validation variables
cv_variables <- list(
    n_hidden_layers = c(1),
    hidden_layer_nodes = seq(2, 6, 1),
    learnrate = c(0.25, 0.5, 1, 2),
    kfolds = 10
)

# Perform CV
best_f1 <- 0
results_df <- data.frame()
for(layers in cv_variables$n_hidden_layers){
    for(nodes in cv_variables$hidden_layer_nodes){
        for(learnrate in cv_variables$learnrate){
            current_f1 <- c()
            best_fold_f1 <- 0
            for(k in 1:cv_variables$kfolds){
                cat("\r", k, "/", cv_variables$kfolds, " folds")
                # Split data into test and cv
                split_data_cv <- random_split(
                    trainset,
                    (cv_variables$kfolds - 1)/cv_variables$kfolds
                )
                trainset_split <- split_data_cv[[1]]
                cvset_split <- split_data_cv[[2]]
                # Train NN
                nn <- neuralnet(
                  label ~ .,
                  data=trainset_split,
                  hidden=rep(nodes, layers),
                  linear.output=FALSE,
                  threshold=learnrate,
                  stepmax = 500000,
                  rep=2
                )
                # Evaluate NN
                nn_f1score <- evaluate_nn(nn, cvset_split)
                if(nn_f1score > best_fold_f1){
                    best_nn_from_fold <- nn
                    best_fold_f1 <- nn_f1score
                }
                current_f1 <- c(current_f1, nn_f1score)
            }
            avg_f1 <- mean(current_f1)
            max_f1 <- max(current_f1)
            min_f1 <- min(current_f1)
            if(avg_f1 > best_f1){
                best_f1 <- avg_f1
                best_nn <- best_nn_from_fold
                cat("\n")
                print("New best model: ")
            }
            # Store results
            current_results <- data.frame(
                layers = layers,
                nodes = nodes,
                learnrate = learnrate,
                avg_f1 = avg_f1,
                max_f1 = max_f1,
                min_f1 = min_f1
            )
            results_df <- rbind(results_df, current_results)
            # Print results
            cat("\n")
            print(
                paste0(
                    "Avg F1: ", round(avg_f1, 3),
                    " best f1: ", round(max_f1,3),
                    ", layers: ", layers,
                    ", nodes: ", nodes,
                    ", learnrate: ", learnrate
                )
            )
        }
    }
}

plot_results <- melt(results_df, id=c("nodes", "layers", "learnrate"))
ggplot(plot_results, aes(x=nodes, y=value, col=variable)) +
    geom_line() +
    theme_bw() +
    facet_wrap(~learnrate) +
    labs(x="Nodes", y="F1 score", col="Score type")
```

The graph above shows our F1 scores for different settings. Our x-axis is the number of nodes, and each plot is a different learning rate. We will only use one hidden layer.

### Evaluation

Here we evaluate our neural network.

```{r evaluate_neuralnet, echo=TRUE}
# Make prediction on test set
best_nn_f1_score_test <- evaluate_nn(best_nn, testset)
```

The F1 score of our best neural network is `r round(best_nn_f1_score_test, 3)`.

It is likely we can do better with our next model.

## XGBoost

XGBoost is a decision tree learning algorithm in R. It is a popular choice in many learning algorithms, and works well with very large data. More information about XGBoost in R can be found [here](https://xgboost.readthedocs.io/en/latest/R-package/index.html).

### Training

To train an XGBoost algorithm we need to specify a number of hyperparameters, such as how quickly should the algorithm learn (too fast and we may not converge, too slow and we might not optimise), how deep can the decision trees be (too shallow and they aren't very informative, too deep and we risk overfitting), and more.

Exacrlt like a neural network, to determine these parameters we can do a grid search, training a new algorithm each time. This means we select a range of options and train a new algorithm for each one. Each time we put aside a little of the training data, and then use it to evaluate the model afterwards. Once we determine the best parameters, we train the full model.

```{r xgboost, echo=TRUE, message=FALSE}
set.seed(25)
# Make train data in XGBoost form
dtrain <- xgb.DMatrix(data = sparse.model.matrix(label ~ .-1, data=trainset), label = renum(trainset$label))
dtest <- xgb.DMatrix(data = sparse.model.matrix(label ~ .-1, data=testset), label = renum(testset$label))

# Define our F1 metric
xgb_eval_f1 <- function (yhat, dtrain) {
  y = getinfo(dtrain, "label")
  return (list(metric = "f1", value = f1Score(y, yhat, cutoff=0.2)))
}

# Define parameters
objective='binary:logistic'
eval_metric = xgb_eval_f1
booster = "gbtree"

# Gridsearch best params
depth <- c(3, 4, 5, 6, 7, 8)
eta <- c(0.05, 0.075, 0.1)

grid_result <- data.frame()
for(d in depth){
    #print(paste0("Depth: ", d))
    for(e in eta){
        #print(paste0("ETA: ", e))
        cv <- xgb.cv(
            data = dtrain,
            nfold = 10,
            nrounds = 500,
            feval = eval_metric,
            max_depth = d,
            eta = e,
            subsample = 0.9,
            maximize = TRUE,
            print_every_n = 50,
            early_stopping_rounds = 100,
            verbose = 0
        )
        eval <- cv$evaluation_log
        best_iter <- eval[which.max(eval$test_f1_mean),]
        grid_result <- rbind(grid_result, data.frame(
                best_iter = best_iter$iter,
                test_f1 = best_iter$test_f1_mean,
                depth = d,
                eta = e
            )
        )
    }
}

ggplot(grid_result, aes(x=depth, col=as.factor(eta), y=test_f1)) +
    geom_line() +
    geom_point(aes(size=best_iter)) +
    theme_bw() +
    labs(x="Depth", y="F1 Score", col="ETA (learning rate)", size="Best iteration")

best_vals <- grid_result[which.max(grid_result$test_f1),]

xgmodel <- xgb.train(
    data =  dtrain,
    nrounds = best_vals$best_iter,
    verbose = 0,
    feval = eval_metric,
    objective = objective,
    max_depth = best_vals$depth,
    eta = best_vals$eta,
    print_every_n = 10,
    booster = booster,
    watchlist = list(train = dtrain),
    maximize = TRUE
)
```

This plot shows the F1 score for a range of options. We can see which depth and ETA (training rate) give the best results, and how many training iterations it took to get there.

For interest, our best depth was `r best_vals$depth`, our best ETA was `r best_vals$eta`, and the F1 score on the cross-validation data was `r round(best_vals$test_f1, 3)`

### Evaluation

```{r evaluate_xgboost, echo=TRUE, message=TRUE}
train_predictions <- predict(xgmodel, dtrain)
threshold_f1 <- function(threshold){
    return(f1Score(renum(trainset$label), ifelse(train_predictions > threshold, 1, 0)))
}
optimum_threshold <- optimize(threshold_f1, interval=c(0,1), maximum = TRUE)
train_predictions <- ifelse(train_predictions > optimum_threshold$maximum, 1, 0)
test_predictions <- ifelse(predict(xgmodel, dtest) > optimum_threshold$maximum, 1, 0)
train_f1_score <- f1Score(renum(trainset$label), train_predictions)
test_f1_score <- f1Score(renum(testset$label), test_predictions)
```

From our XGBoost model we obtain an F1 score on test data of `r round(test_f1_score, 3)`.

### Feature importance

From our trained model we can see which features are most important for determining the application decision. We focus on the gain metric for each feature (the improvement in accuracy brought by a feature to the branches it is on). A high gain score implies that the feature adds a lot of information to determining the outcome.

More information about the gain metric can be found under the Measure feature important section of the [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/R-package/discoverYourData.html).

```{r, echo=TRUE}
xgb.importance(model=xgmodel)
```


# Appendix
## Plots
### Case officer sex

For interest we can plot the application approval rate by the sex of the case worker. It must be reiterated that, as this is an observational study (i.e. it is not a controlled experiment, but just observation of data), no causal effect can be inferred. This means we cannot say that the sex causes the approval rate (or vice versa, although it is unlikely that the approval rate causes someone to have a certain sex).

```{r case_officer_sex_plots, echo=TRUE}
data_plot <- data
data_plot[, Case.Officer.Sex := ifelse(Case.Officer.Male >0.5, "Male", "Female")]
data_plot <- data_plot[label %in% c(0, 1), .(percent = sum(label)*100/.N, cases = .N, success = sum(label)), by=c("Case.Officer.Sex")]
ggplot(data_plot, aes(x=Case.Officer.Sex, y=percent)) +
    geom_bar(stat="identity", fill="Navy") +
    geom_label(aes(label=paste(cases, "cases"))) +
    theme_bw() +
    labs(x="Case officer sex", y="Applications approved (%)", title="Application approval by sex")
```
